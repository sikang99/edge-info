## LLM : Large Language Model 

- VLM : Vision(Video) Language Model
- VNM : Visual Navigation Model
- ALM : Augmented Language Models
- Foundation Model, Big Model


### Articles
- 2023/03/08 [Google’s PaLM-E is a generalist robot brain that takes commands](https://arstechnica.com/information-technology/2023/03/embodied-ai-googles-palm-e-allows-robot-control-with-natural-commands/)
- 2023/03/08 [What is a Large Language Model (LLM)?](https://www.mlq.ai/what-is-a-large-language-model-llm/)
- 2023/03/14 [You can now run a GPT-3 level AI model on your laptop, phone, and Raspberry Pi](https://arstechnica.com/information-technology/2023/03/you-can-now-run-a-gpt-3-level-ai-model-on-your-laptop-phone-and-raspberry-pi/)
- 2023/03/13 [Alpaca: A Strong Open-Source Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- 2023/02/24 [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- 2023/02/16 [Timeline of AI and language models](https://lifearchitect.ai/timeline/)
- 2023/00/00 ----
- 2022/12/15 [Seven research papers push foundation model boundaries](https://snorkel.ai/seven-research-papers-push-foundation-model-boundaries/)
- 2022/07/28 [Grounding Language Models for Spatial Reasoning](https://julenetxaniz.eus/en/project/spatial-reasoning/)
- 2022/06/22 [DeepMind Trains 80 Billion Parameter AI Vision-Language Model Flamingo](https://www.infoq.com/news/2022/06/deepmind-flamingo-vlm/)
- 2022/06/09 [Generalized Visual Language Models](https://lilianweng.github.io/posts/2022-06-09-vlm/)
- 2022/05/02 [Foundation Models and the Future of Multi-Modal AI](https://lastweekin.ai/p/multi-modal-ai)
- 2022/04/28 [Tackling multiple tasks with a single visual language model](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)



### Information
- [OmniXAI](https://opensource.salesforce.com/OmniXAI/latest/index.html]
- [A Catalog of Transformer Models](https://orkg.org/comparison/R385010/)
- [Large Language Model (LLM)](https://primo.ai/index.php?title=Large_Language_Model_(LLM))
- [Standford Alpaca](https://crfm.stanford.edu/alpaca/)
- [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
- [BLIP-2: A new Visual Language Model by Salesforce](https://wandb.ai/gladiator/BLIP-2/reports/BLIP-2-A-new-Visual-Language-Model-by-Salesforce--VmlldzozNjM0NjYz)
- OpenAI [CLIP: Connecting text and images](https://openai.com/research/clip)
- [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/)
- [F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models](https://sites.google.com/view/f-vlm/home)
- [Open-World Object Manipulation using Pre-Trained Vision-Language Models](https://robot-moo.github.io/)
- [Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models](https://semantic-abstraction.cs.columbia.edu/)



### Videos 
- [한국인공지능아카데미](https://www.youtube.com/@aiacademy131)
- [거꾸로 읽는 AI 이야기](https://www.youtube.com/@gokkulearn)
	- [VLM backbone에 대한 트렌드를 알아보자!](https://www.youtube.com/watch?v=NgxSbyoiQYM)


### Open Source
- [Georgelingzj/up-to-date-Vision-Language-Models](https://github.com/Georgelingzj/up-to-date-Vision-Language-Models) - Up-to-date Vision Language Models collection. Mainly focus on computer vision
- [facebookresearch/llama](https://github.com/facebookresearch/llama) - Inference code for LLaMA models
- [tloen/llama-int8](https://github.com/tloen/llama-int8) - Quantized inference code for LLaMA models
- [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) - Port of Facebook's LLaMA model in C/C++
- [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca) - Code and documentation to train Stanford's Alpaca models, and generate the data
- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora) - Code for reproducing the Stanford Alpaca InstructLLaMA result on consumer hardware
- [zengyan-97/X2-VLM](https://github.com/zengyan-97/X2-VLM) - All-In-One VLM: Image + Video + Transfer to Other Languages / Domains
- [ejsalin/vlm-probing](https://github.com/ejsalin/vlm-probing)
- [anysphere/gpt-4-for-code](https://github.com/anysphere/gpt-4-for-code) - Some examples of GPT-4 for code!


### Papers
- 2023 [Foundation Models for Decision Making: Problems, Methods, and Opportunities](https://arxiv.org/abs/2303.04129)
- 2023 [Transformer models: an introduction and catalog](https://arxiv.org/abs/2302.07730)
- 2023 [Human Action Recognition: A Taxonomy-Based Survey, Updates, and Opportunities](https://www.mdpi.com/1424-8220/23/4/2182)
- 2023 [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)
- 2023 [F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models](https://arxiv.org/abs/2209.15639)
- 2022 [Vision-Language Pre-Training: Basics, Recent Advances, and Future Trends](https://www.nowpublishers.com/article/Details/CGV-105)
- 2022 [LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action](https://arxiv.org/abs/2207.04429)
- 2022 [Talking About Large Language Models](https://arxiv.org/abs/2212.03551)
- 2022 [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)
- 2022 [X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks](https://arxiv.org/abs/2211.12402)
- 2022 [Pre-Trained Word Embedding and Language Model Improve Multimodal Machine Translation: A Case Study in Multi30K](https://ieeexplore.ieee.org/document/9803016)
- 2022 [VLMbench: A Benchmark for Vision-and-Language Manipulation](https://embodied-ai.org/papers/2022/6.pdf)
- 2021 [PolyViT: Co-training Vision Transformers on Images, Videos and Audio](https://arxiv.org/abs/2111.12993)
